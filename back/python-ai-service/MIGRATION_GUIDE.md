# üöÄ Gu√≠a de Migraci√≥n: De Ollama a GPT-OSS

Esta gu√≠a te ayudar√° a migrar el chatbot de Ollama (local) a GPT-OSS de OpenAI usando transformers.

## üìã Resumen de Cambios

### Antes (Ollama - Local)
- ‚úÖ Modelo corriendo en tu m√°quina
- ‚úÖ Ollama serv√≠a el modelo v√≠a API REST
- ‚ùå Limitado a modelos soportados por Ollama
- ‚ùå Configuraci√≥n espec√≠fica de Ollama

### Ahora (GPT-OSS - Python Service)
- ‚úÖ Modelo GPT-OSS-120B de OpenAI
- ‚úÖ Servicio Python con Flask + transformers
- ‚úÖ Compatible con cualquier modelo de HuggingFace
- ‚úÖ Mayor flexibilidad y control

## üõ†Ô∏è Instalaci√≥n Paso a Paso

### 1. Instalar Python AI Service

```bash
# Navegar a la carpeta del servicio
cd back/python-ai-service

# Opci√≥n A: Usar script autom√°tico (Windows)
start.bat

# Opci√≥n B: Usar script autom√°tico (Linux/Mac)
chmod +x start.sh
./start.sh

# Opci√≥n C: Manual
python -m venv venv
venv\Scripts\activate  # Windows
# o
source venv/bin/activate  # Linux/Mac

pip install -r requirements.txt
python app.py
```

### 2. Configurar Backend NestJS

Edita tu archivo `.env` en la carpeta `back/`:

```env
# Activar uso del servicio Python
USE_LOCAL_MODEL=true
PYTHON_AI_SERVICE_URL=http://localhost:5000

# Opcional: HuggingFace API como respaldo
HUGGINGFACE_API_KEY=tu_token_aqui
```

### 3. Iniciar Servicios

**Terminal 1 - Python AI Service:**
```bash
cd back/python-ai-service
python app.py
```

**Terminal 2 - Backend NestJS:**
```bash
cd back
npm run start:dev
```

**Terminal 3 - Frontend:**
```bash
cd front
npm run dev
```

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Usar un Modelo M√°s Ligero

Si GPT-OSS-120B es muy pesado para tu m√°quina, edita `app.py` l√≠neas 65 y 72:

```python
# Cambiar de:
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-safeguard-120b")
model = AutoModelForCausalLM.from_pretrained("openai/gpt-oss-safeguard-120b", ...)

# A uno de estos modelos m√°s ligeros:

# Opci√≥n 1: GPT-OSS 7B (~14GB)
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-safeguard-7b")
model = AutoModelForCausalLM.from_pretrained("openai/gpt-oss-safeguard-7b", ...)

# Opci√≥n 2: Microsoft Phi-3 Mini (~8GB) - MUY RECOMENDADO
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3-mini-4k-instruct", ...)

# Opci√≥n 3: Mistral 7B (~14GB)
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2", ...)
```

### Optimizaci√≥n de Memoria

Para reducir el uso de memoria, agrega cuantizaci√≥n 8-bit:

```python
model = AutoModelForCausalLM.from_pretrained(
    "openai/gpt-oss-safeguard-120b",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True,  # ‚Üê Agregar esta l√≠nea
    low_cpu_mem_usage=True,
)
```

### Usar GPU (NVIDIA CUDA)

Si tienes una GPU NVIDIA, instala PyTorch con soporte CUDA:

```bash
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

Verifica que funcione:
```python
import torch
print(torch.cuda.is_available())  # Debe ser True
```

## üß™ Pruebas

### 1. Verificar Python AI Service

```bash
# Health check
curl http://localhost:5000/health

# Respuesta esperada:
# {"status": "ready", "model_loaded": true, "device": "cuda"}
```

### 2. Probar Chat

```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"¬øQu√© es Gender Quest?"}]}'
```

### 3. Probar desde Frontend

1. Abre http://localhost:3000
2. Navega al chat (icono flotante)
3. Escribe una pregunta
4. Verifica que la respuesta viene del modelo GPT-OSS

## üêõ Soluci√≥n de Problemas

### Error: "CUDA out of memory"

**Soluci√≥n 1:** Usa un modelo m√°s peque√±o (ver "Usar un Modelo M√°s Ligero")

**Soluci√≥n 2:** Activa cuantizaci√≥n 8-bit (ver "Optimizaci√≥n de Memoria")

**Soluci√≥n 3:** Reduce `max_new_tokens` en `app.py` l√≠nea 97:
```python
outputs = model.generate(
    **inputs,
    max_new_tokens=200,  # Reducir de 400 a 200
    ...
)
```

### Error: "Connection refused" en backend

Verifica que Python AI Service est√© corriendo:
```bash
curl http://localhost:5000/health
```

Si no responde, inicia el servicio:
```bash
cd back/python-ai-service
python app.py
```

### Modelo tarda mucho en cargar

Es normal para modelos grandes:
- GPT-OSS-120B: 5-10 minutos
- Phi-3-mini: 1-2 minutos

La primera vez tambi√©n descargar√° el modelo (~240GB para 120B, ~8GB para Phi-3).

### Backend usa respuestas demo

Verifica tu `.env`:
```env
USE_LOCAL_MODEL=true  # Debe ser true
PYTHON_AI_SERVICE_URL=http://localhost:5000  # URL correcta
```

Reinicia el backend:
```bash
cd back
npm run start:dev
```

## üìä Comparaci√≥n de Modelos

| Modelo | Tama√±o | Tiempo Carga | RAM/VRAM | Calidad |
|--------|--------|--------------|----------|---------|
| GPT-OSS-120B | ~240GB | 5-10 min | 256GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| GPT-OSS-7B | ~14GB | 2-3 min | 32GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Phi-3-mini | ~8GB | 1-2 min | 16GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Mistral-7B | ~14GB | 2-3 min | 32GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê |

**Recomendaci√≥n:** Usa **Phi-3-mini** para desarrollo (r√°pido, ligero, buena calidad)

## üìö Recursos Adicionales

- [Documentaci√≥n transformers](https://huggingface.co/docs/transformers/)
- [GPT-OSS en HuggingFace](https://huggingface.co/openai/gpt-oss-safeguard-120b)
- [Phi-3 en HuggingFace](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
- [PyTorch CUDA Setup](https://pytorch.org/get-started/locally/)

## üéØ Pr√≥ximos Pasos

1. ‚úÖ Instalar Python AI Service
2. ‚úÖ Configurar `.env`
3. ‚úÖ Probar endpoints
4. ‚úÖ Verificar frontend
5. üöÄ ¬°Listo para usar!

¬øProblemas? Revisa la secci√≥n "Soluci√≥n de Problemas" o abre un issue.
