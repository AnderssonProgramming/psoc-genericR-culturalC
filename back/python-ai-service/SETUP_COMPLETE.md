# üéâ Migraci√≥n Completada: Ollama ‚Üí GPT-OSS

## ‚úÖ Cambios Realizados

### 1. Nuevo Servicio Python AI (`back/python-ai-service/`)
- ‚úÖ `app.py` - Servidor Flask con transformers y GPT-OSS
- ‚úÖ `requirements.txt` - Dependencias Python
- ‚úÖ `README.md` - Documentaci√≥n del servicio
- ‚úÖ `MIGRATION_GUIDE.md` - Gu√≠a detallada de migraci√≥n
- ‚úÖ `start.bat` - Script de inicio autom√°tico (Windows)
- ‚úÖ `start.sh` - Script de inicio autom√°tico (Linux/Mac)
- ‚úÖ `.gitignore` - Ignorar archivos temporales

### 2. Backend NestJS Actualizado
- ‚úÖ `chat.service.ts` - Reemplazado `callOllama()` por `callPythonAiService()`
- ‚úÖ `.env.example` - Actualizado con nuevas variables de entorno

## üöÄ C√≥mo Usar

### Paso 1: Configurar Variables de Entorno

Edita `back/.env` y agrega:

```env
USE_LOCAL_MODEL=true
PYTHON_AI_SERVICE_URL=http://localhost:5000
```

### Paso 2: Instalar y Ejecutar Python AI Service

**Opci√≥n A - Autom√°tico (Recomendado):**

Windows:
```bash
cd back/python-ai-service
start.bat
```

Linux/Mac:
```bash
cd back/python-ai-service
chmod +x start.sh
./start.sh
```

**Opci√≥n B - Manual:**

```bash
cd back/python-ai-service

# Crear entorno virtual
python -m venv venv

# Activar entorno
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt

# Iniciar servicio
python app.py
```

### Paso 3: Iniciar Backend NestJS

```bash
cd back
npm run start:dev
```

### Paso 4: Iniciar Frontend

```bash
cd front
npm run dev
```

## ‚ö†Ô∏è IMPORTANTE: Modelo GPT-OSS-120B

El modelo `openai/gpt-oss-safeguard-120b` es **MUY GRANDE** (~240GB):

- Requiere m√≠nimo **256GB de RAM** o **80GB de VRAM** (GPU)
- Primera descarga puede tomar **horas**
- Tiempo de carga: **5-10 minutos**

### üéØ RECOMENDACI√ìN: Usar Phi-3-mini (M√°s Ligero)

Para desarrollo, es mejor usar un modelo m√°s peque√±o. Edita `back/python-ai-service/app.py`:

**L√≠nea 65:**
```python
# Cambiar de:
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-safeguard-120b")

# A:
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
```

**L√≠nea 72:**
```python
# Cambiar de:
model = AutoModelForCausalLM.from_pretrained("openai/gpt-oss-safeguard-120b", ...)

# A:
model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3-mini-4k-instruct", ...)
```

**Phi-3-mini ventajas:**
- ‚úÖ Solo ~8GB
- ‚úÖ Carga en 1-2 minutos
- ‚úÖ Funciona en laptops normales
- ‚úÖ Excelente calidad para este caso de uso

## üß™ Verificar que Funciona

### 1. Verificar Python AI Service

```bash
curl http://localhost:5000/health
```

Debe responder:
```json
{
  "status": "ready",
  "model_loaded": true,
  "device": "cuda"  // o "cpu"
}
```

### 2. Probar Chat

```bash
curl -X POST http://localhost:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"¬øQu√© es Gender Quest?"}]}'
```

### 3. Probar desde Frontend

1. Abre http://localhost:3000
2. Haz clic en el chatbot flotante
3. Escribe: "¬øQu√© es Gender Quest?"
4. Deber√≠as recibir una respuesta del modelo

## üêõ Soluci√≥n R√°pida de Problemas

### Backend muestra "Python AI Service no disponible"

**Soluci√≥n:** Aseg√∫rate de que el servicio Python est√© corriendo

```bash
cd back/python-ai-service
python app.py
```

### Error "CUDA out of memory"

**Soluci√≥n 1:** Usa Phi-3-mini (ver arriba)

**Soluci√≥n 2:** Activa cuantizaci√≥n 8-bit en `app.py` l√≠nea 73:

```python
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    load_in_8bit=True,  # ‚Üê Agregar esto
    ...
)
```

### Modelo tarda mucho en descargar

**Normal:** Primera vez descarga el modelo completo
- Phi-3-mini: ~4GB (10-30 min)
- GPT-OSS-120B: ~240GB (horas)

Los modelos se cachean en `~/.cache/huggingface/`

## üìö Documentaci√≥n Adicional

- **Gu√≠a Completa:** `back/python-ai-service/MIGRATION_GUIDE.md`
- **README Python:** `back/python-ai-service/README.md`
- **Transformers Docs:** https://huggingface.co/docs/transformers/

## üéØ Pr√≥ximos Pasos

1. ‚úÖ Cambiar a Phi-3-mini (recomendado)
2. ‚úÖ Probar el chatbot
3. ‚úÖ Hacer commit de cambios
4. üöÄ ¬°Disfrutar del nuevo sistema!

## üì¶ Commit Sugerido

```bash
git add .
git commit -m "feat: migrate chatbot from Ollama to GPT-OSS with Python transformers

- Add Python AI Service with Flask + transformers
- Replace Ollama integration with GPT-OSS
- Add automatic setup scripts (start.bat/start.sh)
- Update .env configuration
- Add comprehensive migration guide
- Recommend Phi-3-mini for development"

git push origin main
```

---

**¬øNecesitas ayuda?** Revisa `MIGRATION_GUIDE.md` para soluci√≥n de problemas detallada.
