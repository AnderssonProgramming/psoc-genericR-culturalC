from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Variables globales para el modelo
tokenizer = None
model = None
device = None

SYSTEM_CONTEXT = """Eres un asistente educativo del juego "Gender Quest" sobre roles de g√©nero y equidad.

CONTEXTO DE LA APLICACI√ìN:
- Gender Quest es un quiz interactivo de 10 preguntas sobre roles de g√©nero
- Categor√≠as: Hogar, Sociedad, y Profesional
- Objetivo: Cuestionar estereotipos y promover reflexi√≥n sobre equidad de g√©nero
- Los usuarios responden preguntas y reciben feedback educativo
- Al finalizar obtienen un puntaje y aparecen en un ranking global

TEMAS PRINCIPALES:
1. Roles de g√©nero en el hogar (tareas dom√©sticas, crianza, decisiones)
2. Estereotipos sociales (emociones, apariencia, maternidad/paternidad)
3. Desigualdad en el √°mbito profesional (liderazgo, brecha salarial, oportunidades)

Tu rol es:
‚úÖ Responder preguntas sobre roles de g√©nero, estereotipos y equidad
‚úÖ Explicar conceptos del juego y sus categor√≠as
‚úÖ Proporcionar informaci√≥n educativa y basada en evidencia
‚úÖ Mantener un tono respetuoso, inclusivo y educativo
‚úÖ Responder en espa√±ol de forma concisa (m√°ximo 3-4 p√°rrafos)

‚ùå NO debes:
- Juzgar o criticar las respuestas del usuario
- Dar opiniones pol√≠ticas extremas
- Usar lenguaje ofensivo o discriminatorio"""


def load_model():
    """Carga el modelo GPT-OSS de OpenAI"""
    global tokenizer, model, device
    
    try:
        logger.info("ü§ñ Cargando modelo GPT-OSS-120B de OpenAI...")
        logger.info("‚ö†Ô∏è NOTA: Este modelo es grande (~240GB). Aseg√∫rate de tener suficiente RAM/VRAM.")
        
        # Detectar dispositivo (GPU si est√° disponible)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üì± Usando dispositivo: {device}")
        
        # Cargar tokenizer
        logger.info("üìö Cargando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-safeguard-120b")
        
        # Cargar modelo con configuraci√≥n de memoria optimizada
        logger.info("üß† Cargando modelo (esto puede tomar varios minutos)...")
        model = AutoModelForCausalLM.from_pretrained(
            "openai/gpt-oss-safeguard-120b",
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto",  # Distribuci√≥n autom√°tica en GPU/CPU
            low_cpu_mem_usage=True,
        )
        
        logger.info("‚úÖ Modelo cargado exitosamente")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error cargando modelo: {e}")
        logger.error("üí° Considera usar un modelo m√°s peque√±o como 'openai/gpt-oss-safeguard-7b' o 'microsoft/phi-3-mini-4k-instruct'")
        return False


def generate_response(messages):
    """Genera una respuesta usando el modelo"""
    global tokenizer, model, device
    
    try:
        # Agregar contexto del sistema
        full_messages = [
            {"role": "system", "content": SYSTEM_CONTEXT}
        ] + messages
        
        # Preparar inputs
        inputs = tokenizer.apply_chat_template(
            full_messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(device)
        
        # Generar respuesta
        logger.info("üí≠ Generando respuesta...")
        outputs = model.generate(
            **inputs,
            max_new_tokens=400,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
        
        # Decodificar solo la nueva respuesta
        response = tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[-1]:],
            skip_special_tokens=True
        ).strip()
        
        logger.info("‚úÖ Respuesta generada")
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Error generando respuesta: {e}")
        return get_fallback_response(messages[-1]["content"])


def get_fallback_response(user_message):
    """Respuesta de respaldo si el modelo falla"""
    return """Lo siento, estoy experimentando dificultades t√©cnicas en este momento. 

Gender Quest es un juego educativo sobre roles de g√©nero con 10 preguntas en 3 categor√≠as: Hogar, Sociedad y Profesional. 

¬øPuedes intentar tu pregunta nuevamente o reformularla de otra manera?"""


@app.route('/health', methods=['GET'])
def health_check():
    """Endpoint para verificar el estado del servicio"""
    is_ready = model is not None and tokenizer is not None
    return jsonify({
        "status": "ready" if is_ready else "loading",
        "model_loaded": is_ready,
        "device": str(device) if device else "unknown"
    }), 200 if is_ready else 503


@app.route('/chat', methods=['POST'])
def chat():
    """Endpoint principal para el chat"""
    try:
        data = request.json
        messages = data.get('messages', [])
        
        if not messages:
            return jsonify({"error": "No messages provided"}), 400
        
        # Verificar que el modelo est√© cargado
        if model is None or tokenizer is None:
            return jsonify({
                "error": "Model not loaded yet",
                "message": get_fallback_response(messages[-1].get("content", ""))
            }), 503
        
        # Generar respuesta
        response = generate_response(messages)
        
        return jsonify({
            "message": response,
            "timestamp": None
        }), 200
        
    except Exception as e:
        logger.error(f"‚ùå Error en /chat: {e}")
        return jsonify({
            "error": str(e),
            "message": get_fallback_response("")
        }), 500


@app.route('/info', methods=['GET'])
def info():
    """Informaci√≥n sobre el modelo"""
    return jsonify({
        "model": "openai/gpt-oss-safeguard-120b",
        "status": "ready" if model is not None else "not_loaded",
        "device": str(device) if device else "unknown",
        "framework": "transformers + pytorch"
    })


if __name__ == '__main__':
    # Cargar modelo al iniciar
    logger.info("üöÄ Iniciando servicio de IA...")
    success = load_model()
    
    if not success:
        logger.warning("‚ö†Ô∏è El modelo no se carg√≥ correctamente. El servicio funcionar√° con respuestas de respaldo.")
    
    # Iniciar servidor Flask
    port = 5000
    logger.info(f"üåê Servidor escuchando en http://localhost:{port}")
    app.run(host='0.0.0.0', port=port, debug=False)
