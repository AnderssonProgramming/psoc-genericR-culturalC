# Python AI Service - GPT-OSS

Servicio Python que expone el modelo GPT-OSS-120B de OpenAI mediante una API REST para ser consumido por el backend de NestJS.

## 游 Instalaci칩n

### 1. Crear entorno virtual

```bash
# Windows
python -m venv venv
.\venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 2. Instalar dependencias

```bash
pip install -r requirements.txt
```

**丘멆잺 IMPORTANTE:** El modelo GPT-OSS-120B es muy grande (~240GB). Si no tienes suficiente RAM/VRAM, considera usar un modelo m치s peque침o:

#### Alternativas m치s ligeras:
- `openai/gpt-oss-safeguard-7b` (~14GB)
- `microsoft/phi-3-mini-4k-instruct` (~8GB)
- `mistralai/Mistral-7B-Instruct-v0.2` (~14GB)

Para cambiar el modelo, edita `app.py` l칤nea 65 y 72.

## 游꿢 Uso

### Iniciar el servicio

```bash
python app.py
```

El servicio estar치 disponible en `http://localhost:5000`

### Endpoints

#### 1. Health Check
```bash
GET /health
```

Respuesta:
```json
{
  "status": "ready",
  "model_loaded": true,
  "device": "cuda"
}
```

#### 2. Chat
```bash
POST /chat
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "쯈u칠 es Gender Quest?"}
  ]
}
```

Respuesta:
```json
{
  "message": "Gender Quest es un juego educativo...",
  "timestamp": null
}
```

#### 3. Info
```bash
GET /info
```

## 游댢 Configuraci칩n

### Usar GPU (NVIDIA CUDA)

Si tienes una GPU NVIDIA, el modelo la usar치 autom치ticamente. Aseg칰rate de tener instalado:
- CUDA Toolkit
- PyTorch con soporte CUDA: `pip install torch --index-url https://download.pytorch.org/whl/cu118`

### Ajustar memoria

Si encuentras problemas de memoria, edita `app.py` l칤nea 67-71:

```python
model = AutoModelForCausalLM.from_pretrained(
    "openai/gpt-oss-safeguard-120b",
    torch_dtype=torch.float16,  # Cambiar a float8 o int8 para menor memoria
    device_map="auto",
    load_in_8bit=True,  # Agregar esta l칤nea para quantizaci칩n 8-bit
)
```

## 游냍 Troubleshooting

### Error: "Out of memory"
- Usa un modelo m치s peque침o
- Activa quantizaci칩n 8-bit agregando `load_in_8bit=True`
- Reduce `max_new_tokens` en l칤nea 97

### Error: "CUDA not available"
- Instala PyTorch con soporte CUDA
- O usa CPU (ser치 m치s lento)

### Modelo tarda mucho en cargar
- Es normal para modelos grandes (puede tomar 5-10 minutos)
- Considera usar un modelo m치s peque침o

## 游닇 Notas

- Primera carga descargar치 el modelo (puede tomar horas dependiendo de tu conexi칩n)
- Los modelos se cachean en `~/.cache/huggingface/`
- Para producci칩n, considera usar servicios administrados como HuggingFace Inference API
